{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Coding of natural images and MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olshausen and Field, in their landmark paper in Nature, \"Emergence of simple-cell receptive field properties by learning a sparse code for natural images\", propose a method for representing images using a dictionary. \n",
    "\n",
    "One representation we have already worked with is PCA. In Assignment 2 Problem 3, we expressed each image as a combination of the top-$k$ principal vectors of the data. However, principal components are not localized. When we look at the principal components as images, we cannot visually isolate the feature they are representing. The Olshausen & Field sparse coding method attempts to solve this problem by generating a dictionary of localized \"features\" and express each image as a combination of as few of them as possible. \n",
    "\n",
    "Let $y^{(i)}\\in\\mathbb{R}^n$, for $i=1,\\dots,N$ be the set of input images represented as $n$-dimensional vectors. We want to build a dictionary of size $p$, $X\\in\\mathbb{R}^{n\\times d}$, where each column corresponds to an image representing a local feature. Let $\\beta^{(i)}\\in\\mathbb{R}^d$ be the coefficient vector for image $i$. \n",
    "\n",
    "We optimize using the squared loss:\n",
    "$$\\begin{align}\n",
    "\\min_{\\beta, X} & \\;\\; \\sum_{i=1}^N \\left\\{\\frac{1}{2n}\\left\\| y^{(i)} - X \\beta^{(i)} \\right\\|_2^2 + \\lambda \\cdot \\text{SparsityPenalty}(\\beta^{(i)}) \\right\\}\\\\\n",
    "\\text{such that} & \\;\\; \\| X_j\\|_2 \\leq 1\n",
    "\\end{align}$$\n",
    "\n",
    "The features/basis vectors in the dictionary $X$ need not be orthogonal. Ideally, we would optimize both varibles $\\beta$ and $X$ together, however the problem is not jointly convex. But for a fixed $X$, the problem is convex in $\\beta$, and vice versa.\n",
    "\n",
    "In this lab, we alternately update $X$ and $\\beta$. In each iteration, we use stochastic gradient descent to update $X$, then fit $\\beta$ while keeping $X$ fixed. Details of the algorithm are in the assignment writeup. In the following sections, we will discuss methods to fit the coefficients $\\beta$ for fixed $X$.\n",
    "\n",
    "\n",
    "====================================================================================================================\n",
    "__Summary of Notation__\n",
    "\n",
    "* $N$ data points\n",
    "\n",
    "\n",
    "* each data point is in $\\mathbb{R}^n$\n",
    "\n",
    "\n",
    "* $d$ atoms (codewords) in the dictionary (codebook)\n",
    "\n",
    "\n",
    "* overcomplete: $d \\gg n$ i.e. more atoms than input dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part discusses one way to update $\\beta$ when $X$ is fixed.\n",
    "\n",
    "* The SparsityPenalty function in the above optimization problem tries to enforce zero-values in the vectors $\\beta^{(i)}$. \n",
    "\n",
    "\n",
    "* In literature, $0$-\"norm\" of a vector is defined as the number of nonzeros in it, which is the most SparsityPenalty function. \n",
    "\n",
    "\n",
    "* However, using the $0$-\"norm\" makes the problem very difficult to solve. (ASIDE: It's a NP-hard discrete problem). In general, continuous approximations/relaxations of it are used.\n",
    "\n",
    "\n",
    "* Recall the (squared) $2$-norm regularizer: $$\\|\\beta\\|^2_2 = \\sum_{i=1}^d \\beta_i^2$$\n",
    "\n",
    "  It penalize large absolute values of $\\beta_j$s, but the penalty becomes small rapidly as the non-zero values go closer to zero. That means, small components of $\\beta$ don't really pay!\n",
    "\n",
    "\n",
    "* In this sense, its better to use $1$-norm regularizer: $$\\|\\beta\\|_1 = \\sum_{i=1}^d \\left| \\beta_i \\right |$$\n",
    "\n",
    "  Both large and small components of $\\beta$ have to pay in this case.\n",
    "  \n",
    "  *Hint: think about the gradients of both regularizers. If you use l2 regularization, every iteration you deduct $ \\eta \\cdot \\beta$, so larger compoenents shrink more (pay more). If you use l1 regularization, you'll deduct $\\eta \\cdot sgn(\\beta)$ -- all components pay the same.*\n",
    "  \n",
    "  \n",
    "* Thus we have the so-called **Lasso** method: \n",
    "\n",
    "    \\begin{align}\n",
    "    \\min_{\\beta^{(i)} }& \\;\\; \\frac{1}{2n}\\left\\| y^{(i)} - X \\beta^{(i)} \\right\\|_2^2 + \\lambda \\cdot \\|\\beta^{(i)}\\|_1 \\\\\n",
    "    \\end{align}\n",
    "\n",
    "   This is for one data point $y^{(i)}$, you will need to do this for all data points in Assignment 3.\n",
    "\n",
    "\n",
    "* $1$-norm regularization is used to promote sparsity at other places, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Orthogonal Matching Pursuit (OMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Matching pursuit is another algorithm to sparsely fit multidimensional vectors (in our case, $y^{(i)}$) using a dictionary ($X$) of vectors taken from the same space ($\\mathbb{R}^n$).\n",
    "\n",
    "\n",
    "* The objective is achieved using a greedy strategy. Basis vectors (columns of $X$) from the dictionary are chosen in the order of their correlation with the residual $r = y^{(i)} - X \\beta^{(i)}$ step by step, until the approximation is good enough. \n",
    "\n",
    "\n",
    "* In other words, in each step, the residual $r$ (which is initially equal to the data vector) is updated by subtracting its projection onto one atom of the dictionary, with which it has the highest correlation. Next step, you select another atom from the rest. (Why only the rest?)\n",
    "\n",
    "  (_You'll get one atom from the rest, but you don't necessarily need to worry this in the computation._)\n",
    "\n",
    "\n",
    "* The word __orthogonal__ means that _after each step of the OMP algorithm, residual is orthogonal to all the selected columns_.\n",
    "\n",
    "====================================================================================================\n",
    "\n",
    "*__Algorithm: Orthogonal matching pursuit__*\n",
    "\n",
    "Input : data vector $y$, dictionary $X$, number of atoms (basis vectors) you want to choose $k$\n",
    "\n",
    "Output : indices of chosen atoms $S$, coefficient vector $\\beta$\n",
    "\n",
    "- Initialize $r = y$, $\\beta = 0$\n",
    "- Initialize empty set $S$\n",
    "- for $i = 1,\\ldots, k$\n",
    "    - Add $j^* = \\arg\\max_{j} |r^\\top X_j| / \\| X_j \\|_2$ to $S$\n",
    "    - Set $\\beta_{j^*} = r^\\top X_{j^*}$ and $r = r - \\beta_{j^*} X_{j^*}$. A better but slightly more expensive approach is to refit $y$ here.\n",
    "- (Optional, refit $y$ if you didn't refit in the loop.)\n",
    "- return : $S, \\; \\beta$\n",
    "\n",
    "*(Refitting: solve least squares regression on selected basis $\\|y- X_S \\beta_S\\|^2$, obtain new $r$ and $\\beta$)*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
