{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Spark Locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-6a4d8a25bf1f>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-6a4d8a25bf1f>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    print df.rdd.map(lambda x: re.split('(?<=[.!?]) +', x['text'])).collect()\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Andy Ham aham\n",
    "#Problem 4A\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "spark  = SparkSession.builder.master('local').appName('SOU').getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/sou/speeches.json')\n",
    "\n",
    "print df.rdd.map(lambda x: re.split('(?<=[.!?]) +', x['text'])).collect()\n",
    "#print df.rdd.map(lambda x: len(x['text'].split('. '))).collect()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-a7a0d117621b>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-a7a0d117621b>\"\u001b[1;36m, line \u001b[1;32m26\u001b[0m\n\u001b[1;33m    print \"Format is: (Year, # of sentences, mean sentence length in words) \\n\"\u001b[0m\n\u001b[1;37m                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Problem 4B\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "spark  = SparkSession.builder.master('local').appName('SOU').getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/sou/speeches.json')\n",
    "\n",
    "num_sentences = df.rdd.map(lambda x: len(re.split('(?<=[.!?]) +', x['text']))).collect()\n",
    "#num_sentences = df.rdd.map(lambda x: len(x['text'].split('. '))).collect()\n",
    "\n",
    "mean_sentence_len = df.rdd.map(lambda x: len(x['text'].split(' '))/len(re.split('(?<=[.!?]) +', x['text']))).collect()\n",
    "#mean_sentence_len= df.rdd.map(lambda x: len(x['text'].split(' '))/len(x['text'].split('. '))).collect()\n",
    "\n",
    "yearsX = df.rdd.map(lambda x: int(x['year'])).collect()\n",
    "\n",
    "#print num_sentences\n",
    "#print mean_sentence_len\n",
    "#print yearsX\n",
    "\n",
    "combined = df.rdd.map(lambda x: (int(x['year']), len(re.split('(?<=[.!?]) +', x['text'])), len(x['text'].split(' '))/len(re.split('(?<=[.!?]) +', x['text'])))).collect()\n",
    "\n",
    "print \"Format is: (Year, # of sentences, mean sentence length in words) \\n\"\n",
    "print sorted(combined)\n",
    "\n",
    "b1_hat_num = np.cov(yearsX, num_sentences)[0][1] / np.cov(yearsX, num_sentences)[0][0]\n",
    "b0_hat_num = (sum(num_sentences)/len(num_sentences)) - (b1_hat_num * (sum(yearsX)/len(yearsX)))\n",
    "\n",
    "b1_hat_mean = np.cov(yearsX, mean_sentence_len)[0][1] / np.cov(yearsX, mean_sentence_len)[0][0]\n",
    "b0_hat_mean = (sum(mean_sentence_len)/len(mean_sentence_len)) - (b1_hat_mean * (sum(yearsX)/len(yearsX)))\n",
    "\n",
    "y_val1790_num = (b1_hat_num*1790) + b0_hat_num\n",
    "y_val2013_num = (b1_hat_num*2013) + b0_hat_num\n",
    "\n",
    "y_val1790_mean = (b1_hat_mean*1790) + b0_hat_mean\n",
    "y_val2013_mean = (b1_hat_mean*2013) + b0_hat_mean\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(yearsX, num_sentences, '.')\n",
    "plt.plot([1790, 2013], [y_val1790_num, y_val2013_num]) #regression line\n",
    "plt.title('Number of sentences by year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('# of sentences')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(yearsX, mean_sentence_len, '.')\n",
    "plt.plot([1790, 2013], [y_val1790_mean, y_val2013_mean]) #regression line\n",
    "plt.title('Average sentence length by year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average sentence length (# of words)')\n",
    "\n",
    "print \"\\n\\nThe # of sentences in a SOU address has an upward trend with time, whereas the mean sentence length of \"\n",
    "print \"of sentences goes down. It makes sense that there would be generally be an inverse proportion between the \"\n",
    "print \"number of sentences and the average sentence length if the addresses were to have the same amount of content\"\n",
    "print \"Also noted in 4c, from 1801 to 1912 SOU addresses were written, which makes it easier for presidents to have\"\n",
    "print \"less number of sentences/longer average sentence length thanks to semi colons and flowing prose. It's hard to\"\n",
    "print \"do that with oral speeches, and succint phrases are easier for people to pay attention and understand.\"\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-4-3cdf2f670d84>, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-3cdf2f670d84>\"\u001b[1;36m, line \u001b[1;32m50\u001b[0m\n\u001b[1;33m    print \"SOU addresses from 1790 to 1912 experience a clearly upward trend in the number of words, from a\"\u001b[0m\n\u001b[1;37m                                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "#Problem 4C compute 2 regressions of TOTAL NUM WORDS in SOU vs 1790-1912, and 1913-2013\n",
    "%matplotlib inline\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "spark  = SparkSession.builder.master('local').appName('SOU').getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/sou/speeches.json')\n",
    "\n",
    "num_words= df.rdd.map(lambda x: (int(x['year']), len(x['text'].split(' ')))).collect()\n",
    "\n",
    "yearsX = sorted(df.rdd.map(lambda x: int(x['year'])).collect())\n",
    "\n",
    "num_words.sort(key=lambda x: x[0])\n",
    "# tuples of (year, wordcount)\n",
    "\n",
    "before = num_words[:123]\n",
    "after = num_words[123:]\n",
    "\n",
    "before_year, before_words = zip(*before)\n",
    "after_year, after_words= zip(*after)\n",
    "\n",
    "b1_hat_before = np.cov(before_year, before_words)[0][1] / np.cov(before_year, before_words)[0][0]\n",
    "b0_hat_before = (sum(before_words)/len(before_words)) - (b1_hat_before * (sum(before_year)/len(before_year)))\n",
    "\n",
    "b1_hat_after = np.cov(after_year, after_words)[0][1] / np.cov(after_year, after_words)[0][0]\n",
    "b0_hat_after = (sum(after_words)/len(after_words)) - (b1_hat_after * (sum(after_year)/len(after_year)))\n",
    "\n",
    "y_val1790_before= (b1_hat_before*1790) + b0_hat_before\n",
    "y_val1912_before = (b1_hat_before*1912) + b0_hat_before\n",
    "\n",
    "y_val1913_after = (b1_hat_after*1913) + b0_hat_after\n",
    "y_val2013_after = (b1_hat_after*2013) + b0_hat_after\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(before_year, before_words, '.')\n",
    "plt.plot([1790, 1912], [y_val1790_before, y_val1912_before]) #regression line\n",
    "plt.title('# of words in SOU by year 1790-1912')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('# of words')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(after_year, after_words, '.')\n",
    "plt.plot([1913, 2013], [y_val1913_after, y_val2013_after]) #regression line\n",
    "plt.title('# of words in SOU by year 1913-2013')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('# of words')\n",
    "\n",
    "print \"SOU addresses from 1790 to 1912 experience a clearly upward trend in the number of words, from a\" \n",
    "print \"couple thousand to over 25,000 words. However, from 1913 to 2013, the # of words is relatively constant\"\n",
    "print \"around 5 or 6k. This disparity arises from when Thomas Jefferson in 1801 thought giving a speech in person\"\n",
    "print \"to be too monarchial and instead wrote the address and sent it to Congress to be read by a clerk.\"\n",
    "print \"In 1913, Woodrow Wilson re-established the practice of giving the addresses in person. It seems easier\"\n",
    "print \"to add more content while writing an address, since the president is somewhat detached from the actual\"\n",
    "print \"delivery of the speech, but giving an address the president needs to be aware of how long his speech is\"\n",
    "print \"Wilson's first SOU address was probably the model/template for all future addresses, and the following\"\n",
    "print \"presidents tried to keep a similar time/word limit. The exceptions are in 1981 when Jimmy Carter wrote in\"\n",
    "print \"the longest SOU address ever, and another outlier by Harry Truman when he wrote in his address in 1946.\"\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-5-21d0c8f29a80>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-21d0c8f29a80>\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    print \"President with the longest sentences on average is: \"\u001b[0m\n\u001b[1;37m                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "#PROBLEM 4D\n",
    "#Which President has the longest sentences on average? Which has the shortest sentences?\n",
    "#Compute the median, 25% and 75% quantiles across all Presidents. What was\n",
    "#the longest and shortest sentence ever spoken (or written) in a SOU?\n",
    "#numwords/numsentences\n",
    "%matplotlib inline\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "spark  = SparkSession.builder.master('local').appName('SOU').getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/sou/speeches.json')\n",
    "\n",
    "\n",
    "speech = df.rdd.map(lambda x: (x['president'], int(x['year']), len(x['text'].split(' ')),re.split('(?<=[.!?]) +', x['text']))).collect()\n",
    "# (u'James Monroe', 1821, 5765words, 169sentenc), ...\n",
    "\n",
    "speechavglen = df.rdd.map(lambda x: (x['president'], len(x['text'].split(' '))/len(re.split('(?<=[.!?]) +', x['text'])))).reduceByKey(add).collect()\n",
    "speechnumyears = df.rdd.map(lambda x: (x['president'], 1)).reduceByKey(add).collect()\n",
    "\n",
    "\n",
    "final = spark.sparkContext.parallelize(speechavglen + speechnumyears).reduceByKey(lambda a,b: a/b).collect()\n",
    "#list of (president, avg wordcount per sentence) tuples\n",
    "\n",
    "sortedfinal = sorted(final, key=lambda x: x[1])\n",
    "\n",
    "#print sortedfinal\n",
    "print \"President with the longest sentences on average is: \"\n",
    "print sortedfinal[len(sortedfinal)-1]\n",
    "print \"\\nPresident with the shortest sentences on average is: \"\n",
    "print sortedfinal[0]\n",
    "print \"\\nPresident with the median is: \"\n",
    "print sortedfinal[len(sortedfinal)/2]\n",
    "print \"\\nPresident at the 25th percentile is: \"\n",
    "print sortedfinal[len(sortedfinal)*1/4]\n",
    "print \"\\nPresident at the 75th percentile is: \"\n",
    "print sortedfinal[len(sortedfinal)*3/4]\n",
    "\n",
    "def longest_sentence(sentences):\n",
    "    maxlength = 0\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split(' ')) > maxlength:\n",
    "            maxlength = len(sentence.split(' '))\n",
    "            maxs = (sentence, maxlength)\n",
    "    return maxs\n",
    "\n",
    "def shortest_sentence(sentences):\n",
    "    minlength = 1000\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split(' ')) < minlength:\n",
    "            minlength =len(sentence.split(' '))\n",
    "            mins = (sentence, minlength)\n",
    "    return mins\n",
    "\n",
    "shortest = df.rdd.map(lambda x: shortest_sentence(re.split('(?<=[.!?]) +', x['text']))).collect()\n",
    "longest = df.rdd.map(lambda x: longest_sentence(re.split('(?<=[.!?]) +', x['text']))).collect()\n",
    "\n",
    "\n",
    "#need to account for \" mark\n",
    "longest2 = sorted(longest)[len(longest)-1][0].split('\\\"')[1]\n",
    "\n",
    "print \"\\nLongest sentence in SOU: \"\n",
    "print (longest2, len(longest2.split(\" \")))\n",
    "print \"\\nShortest sentence in SOU (considering 2 word count to be minimum, there are a few): \"\n",
    "#print sorted(shortest, key=lambda x: x[1])[156]\n",
    "print sorted(shortest, key=lambda x: x[1])[149]\n",
    "                                \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
